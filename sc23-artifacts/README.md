# Artifact Description (SC23)

## Reproducibility of Experiments


### Details about environment used for experiments

In this section, we provide the detailed installation used when running our experiments. These details can help reproduce exactly the environment. However, our results should not be sensitive to little changes.

The software used on the Polaris supercomputer at the Argonne Leadership Computing Facility for ou Hyperparameter optimization benchmarks is provided in:
- `infos/polaris-os.out`: information about the OS of the system.
- `infos/polaris-env.out`: information about the modules, conda and spack environments.

These files were generated by executing `./infos.sh polaris` from our loaded environment.

### Installation on Polaris at ALCF

On all the systems of the Argonne Leadership Computing Facility (ALCF) we used the `/lus/grand/projects` filesystem to have enough storage space for our installation as the user home directory `~/` is relatively small. The following set of instructions should be run from the location where the environment will be installed:

```console
git clone https://github.com/deephyper/scalable-bo.git
cd scalable-bo/
mkdir build && cd build/
../install/polaris.sh
```

This installation will create the `build/activate-dhenv.sh` script which can be sourced (e.g., from `scalable-bo` root directory `. ./build/activate-dhenv.sh`) to load the installed environment (i.e., activate Conda and Spack environments, also load the Redis configuration `redis.conf`).

The details about the installation steps are provided with comments in `scalable-bo/install/polaris.sh`.

The script will install:
- `deephyper`: Python package used for the Hyperparameter optimization algorithms and distribution of task evaluations. DeepHyper is installed with `[default,mpi,redis]` extra requirements. See [Install DeepHyper with pip](https://deephyper.readthedocs.io/en/latest/install/pip.html) to have details about all extra requirements.
- `deephyper-benchmark`: A library of benchmark for Hyperparameter optimization and other types of similar problems. See more details about these benchmarks at [deephyper/benchmark](https://github.com/deephyper/benchmark). In our script we specifically install the `ECP-Candle/Pilot1/Combo` benchmark used in the paper.
- `scalable-bo`: The code used to run our experiments.

### Running Experiments on Polaris at ALCF

On Polaris the experiments are run from `scalable-bo/experiments/polaris/dhbv2`. Go to the root directory of our experiments:

```console
cd scalable-bo/experiments/polaris/dhbv2
```

The outputs of the experiments will be stored in `scalable-bo/experiments/polaris/dhbv2/output` with a naming that is similar to job scripts. Create this directory: 

```console
mkdir output
```

In all our job scripts (`*.sh`) the `-A datascience` project is used. Replace this value by your own project allocation (e.g., `myprojectname`) with the following command:

```console
cd scalable-bo/experiments/polaris/dhbv2
sed -i '' 's/#PBS -A datascience/#PBS -A myprojectname/' *.sh
```

Now, in order to test the installation the following commands will submit a small debug jobs on two Polaris nodes::

```console
qsub dhb_combo-CBO-RF-UCB-2-3500-debug.sh
```

Then all experiments can be submitted similarly with `qsub`. Each job file is named according to `BenchmarkName-Algorithm-SurrogateModel-AcquisitionFunction-MultifidelityStrategy-NumberOfNodes-SearchDuration.sh` for example `dhb_combo-DBO-RF-UCB-SHA-480-10800.sh` corresponds to:
- `BenchmarkName = dhb_combo`
- `Algorithm = DBO` (Distributed Bayesian Optimization)
- `SurrogateModel = RF` (Random Forest)
- `AcquisitionFunction = UCB` (Upper Confidence Bound)
- `MultifidelityStrategy = SHA` (Successive Halving)
- `NumberOfNodes = 480`
- `SearchDuration = 10800` (in secondes)

Each experiment will create a new folder with similar naming inside `output`. Each of this result folder will contain:
- `results.csv` the main file which contains the metadata of each evaluated hyperparameter configurations (1 evaluation = 1 row).
- `context.yaml` metadata of the executed search (e.g., number of workers, problem description, parameters of the search).
- `deephyper*.log` some log file providing detailed information about the execution.

### Creating Figures

After executing the experiments, we use a semi-automated script to plot our figures `python -m scalbo.plot`. The `plot.yaml` file contains all the metadata necessary to describe the plots. The beginning of  this file `plot.yaml` describes where the experiments results are located as well as where the generated figures should be stored. Replace them with you current directory:

```console
sed -i '' "s_data-root: .*_data-root: $PWD/output_" plot.yaml
sed -i '' "s_data-root: .*_data-root: $PWD/figures" plot.yaml
```

Now the plots should able to be generated with:

```console
python -m scalbo.plot --config plot.yaml
```




